{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "\n",
    "- 대수학에서 텐서(Tensor)란 수를 담을 수 있는 다차원 배열이다.\n",
    "- 텐서는 스칼라, 벡터, 행렬, 그 이상의 차원을 가진 구조체를 모두 아울러 부르는 말이다.\n",
    "- 텐서 안의 어떤 원소의 위치를 나타내기 위해 필요한 인덱스의 개수를 랭크(rank)라 한다.\n",
    "  - 스칼라(Scalar): 랭크가 0인 0차원 텐서이다.\n",
    "    - 예를 들어 숫자 $3.14159$.\n",
    "  - 벡터(Vector): 랭크가 1인 1차원 텐서이다.\n",
    "    - 예를 들어 $[1, 2, 3]$.\n",
    "  - 행렬(Matrix): 랭크가 2인 2차원 텐서이다.\n",
    "    - 예를 들어 $\\begin{bmatrix}1 & 2 \\\\3 & 4\\end{bmatrix}$.\n",
    "  - 고차원 텐서(Higher-order tensors): 랭크 3 이상의 텐서이다. 여러 차원의 값의 배열이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch 패키지는 텐서 객체와 연산을 지원한다.\n",
    "- `torch.tensor()` 메서드로 텐서 객체를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 값이 하나 뿐인 스칼라 텐서는 값이 하나만 들어있는 배열을 인수로 넣어 정의할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.1416])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor([3.14159])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1차원 텐서는 배열을 인수로 넣어 생성할 수 있다.\n",
    "- 파이선의 리스트와 동일하게 음수 인덱싱이 가능하며, 슬라이스도 할 수 있다.\n",
    "- 텐서는 가변형이다. 인덱싱을 통해 저장된 값을 수정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t[0] :  tensor(1)\n",
      "t[3] :  tensor(4)\n",
      "t[-1] :  tensor(4)\n",
      "t[-4] :  tensor(1)\n",
      "t[:2] :  tensor([1, 2])\n",
      "t[1:2] :  tensor([2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 3, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1, 2, 3, 4])\n",
    "print(\"t[0] : \", t[0])\n",
    "print(\"t[3] : \", t[3])\n",
    "# print(\"t[4] : \", t[4]) # IndexError\n",
    "print(\"t[-1] : \", t[-1])\n",
    "print(\"t[-4] : \", t[-4])\n",
    "# print(\"t[-5] : \", t[-5]) # IndexError\n",
    "print(\"t[:2] : \", t[:2])\n",
    "print(\"t[1:2] : \", t[1:2])\n",
    "\n",
    "t[0] = 0\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2차원 텐서는 배열의 배열을 `data`인수로 넣어 정의할 수 있다.\n",
    "- 다음 코드는 $\\begin{bmatrix}1&2&3\\\\4&5&6\\end{bmatrix}$을 정의한다.\n",
    "- 행렬을 나타내는 텐서는 `t[index_row]`로 행 벡터 텐서를 구할 수 있다.\n",
    "- 행렬을 나타내는 텐서는 `t[index_row, index_column]`으로 각 원소의 스칼라 텐서를 구할 수 있다.\n",
    "- 행렬을 나타내는 텐서는 `t[:, index_column]`으로 열 벡터 텐서를 슬라이스 할 수 있다.\n",
    "- 행렬을 나타내는 텐서는 `t[a:b, c:d]`로 $(b-a) \\times (d-c)$ 크기의 부분 행렬을 슬라이스 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t : \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "1행 t[0] :  tensor([1, 2, 3])\n",
      "2행 t[1] :  tensor([4, 5, 6])\n",
      "1행 1열 t[0, 0] :  tensor(1)\n",
      "2행 3열 t[1, 2] :  tensor(6)\n",
      "1열 t[:,0] :  tensor([1, 4])\n",
      "2열 t[:,1] :  tensor([2, 5])\n",
      "3열 t[:,2] :  tensor([3, 6])\n",
      "부분 행렬 t[0:1, 1:3] :  tensor([[2, 3]])\n"
     ]
    }
   ],
   "source": [
    "# 1 2 3\n",
    "# 4 5 6\n",
    "t = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(\"t : \", t, sep='\\n')\n",
    "print(\"1행 t[0] : \", t[0]) # 1행\n",
    "print(\"2행 t[1] : \", t[1]) # 2행\n",
    "print(\"1행 1열 t[0, 0] : \", t[0, 0]) # 1행 1열\n",
    "print(\"2행 3열 t[1, 2] : \", t[1, 2]) # 2행 3열\n",
    "print(\"1열 t[:,0] : \", t[:,0]) # 1열\n",
    "print(\"2열 t[:,1] : \", t[:,1]) # 2열\n",
    "print(\"3열 t[:,2] : \", t[:,2]) # 3열\n",
    "print(\"부분 행렬 t[0:1, 1:3] : \", t[0:1, 1:3]) # (1 x 2) 크기의 부분 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단위 행렬을 만들고 싶다면 `torch.eye`함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], size=(0, 0))\n",
      "tensor([[1.]])\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# print(torch.eye(-1)) # RuntimeError\n",
    "print(torch.eye(0))\n",
    "print(torch.eye(1))\n",
    "print(torch.eye(2))\n",
    "print(torch.eye(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 행렬 텐서를 정의할 경우, 각 행의 길이는 서로 같아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 1 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([\n\u001b[0;32m      2\u001b[0m     [\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m],\n\u001b[0;32m      3\u001b[0m     [\u001b[39m4\u001b[39;49m, \u001b[39m5\u001b[39;49m]\n\u001b[0;32m      4\u001b[0m ])\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 2)"
     ]
    }
   ],
   "source": [
    "# t = torch.tensor([\n",
    "#     [1, 2, 3],\n",
    "#     [4, 5]\n",
    "# ]) # ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tensor()`의 `dtype` 인수로 데이터의 형식을 직접 지정할 수도 있다.\n",
    "- `dtype=None`인 경우 데이터로 부터 `dtype`을 추론한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) torch.int64\n",
      "tensor([1., 2., 3.]) torch.float32\n",
      "tensor([1.0000, 2.0000, 2.4000]) torch.float32\n",
      "tensor([ True, False]) torch.bool\n",
      "tensor([False,  True,  True]) torch.bool\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor() # TypeError: tensor() missing 1 required positional arguments: \"data\"\n",
    "print(torch.tensor([1,2,3]), torch.tensor([1,2,3]).dtype)\n",
    "print(torch.tensor([1.0,2.0,3.0]), torch.tensor([1.0,2.0,3.0]).dtype)\n",
    "print(torch.tensor([1,2,2.4]), torch.tensor([1,2,2.4]).dtype)\n",
    "print(torch.tensor([True,False]), torch.tensor([True, False]).dtype)\n",
    "print(torch.tensor([0,1,2], dtype=torch.bool), torch.tensor([1,2,3], dtype=torch.bool).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tensor()`의 `device` 인수로 텐서가 배정될 장치를 결정할 수 있다.\n",
    "- `device=None`인 경우 `torch.set_default_device()`로 미리 설정해둔 기본값을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([1,2,3]).device)\n",
    "print(torch.tensor([1,2,3], device=\"cpu\").device)\n",
    "print(torch.tensor([1,2,3], device=\"cuda\").device) # cuda를 지원하는 GPU가 없다면 여기서 에러가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.Tensor()` 생성자를 직접 호출하여 텐서를 생성할 수도 있으나,\n",
    "- `torch.Tensor()`는 입력받은 데이터의 형식에 상관없이 `dtype`이 `torch.FloatTensor()`로 고정된다.\n",
    "- 그런 이유로 `torch.tensor()`를 사용하는 편이 더 낫다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) torch.int64\n",
      "tensor([1., 2., 3.]) torch.float32\n",
      "tensor([1., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = [1,2,3]\n",
    "tensor = torch.tensor(data)\n",
    "Tensor = torch.Tensor(data)\n",
    "print(tensor, tensor.dtype)\n",
    "print(Tensor, Tensor.dtype)\n",
    "print(torch.Tensor([True, False]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텐서의 형태는 `tensor.shape`로 구할 수 있다.\n",
    "- 형태는 각 차원의 길이를 순서대로 쓴 것이다.\n",
    "- 예를 들어 다음 코드에 정의한 텐서 `t`는 랭크 3의 텐서이며, 그 형태는 $2 \\times 3 \\times 4$이다. \n",
    "  - 3차원의 길이는 2이다.\n",
    "  - 2차원의 길이는 3이다.\n",
    "  - 1차원의 길이는 4이다.\n",
    "- 차원은 상황에 따라 축(axis)이라고도 부른다.\n",
    "- 차원의 1차원과 2차원은 각각 행렬의 열과 행을 나타내기도 한다.\n",
    "- 인덱싱을 할 때는 고차원의 인덱스부터 순서대로 쓴다.\n",
    "- 예를 들어 텐서 `t`의 첫번째 행렬의 2행 3열 원소는 `t[0, 1, 2]`이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "t = torch.Tensor([[\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12],\n",
    "],\n",
    "[\n",
    "    [-1,-2,-3,-4],\n",
    "    [-5,-6,-7,-8],\n",
    "    [-9,-10,-11,-12],\n",
    "]\n",
    "])\n",
    "print(t.shape)\n",
    "print(t[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텐서에 속한 모든 스칼라 원소의 개수는 `torch.numel()`로 구할 수 있다.\n",
    "- 텐서에 속한 원소를 다차원 인덱스가 아니라 메모리 상에 나타나는 1차원 배열의 순서로 인덱싱하려면 `torch.take()`를 쓸 수 있다.\n",
    "- 예를 들어 다음 코드에서 `t`는 24개의 스칼라를 가지고 있으며, `take`는 $[0,24)$사이의 인덱스를 받을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "tensor(-1.)\n",
      "tensor([ 9., -1., -5.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.Tensor([[\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12],\n",
    "],\n",
    "[\n",
    "    [-1,-2,-3,-4],\n",
    "    [-5,-6,-7,-8],\n",
    "    [-9,-10,-11,-12],\n",
    "]\n",
    "])\n",
    "print(torch.numel(t))\n",
    "print(torch.take(t, torch.tensor(12)))\n",
    "print(torch.take(t, torch.tensor([8, 12, 16])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.empty(shape)`를 통해 값을 초기화하지 않은 텐서를 만들 수 있다.\n",
    "- `torch.zeros(shape)`를 통해 $0$으로 가득한 텐서를 만들 수 있다.\n",
    "- `torch.ones(shape)`를 통해 $1$로 가득한 텐서를 만들 수 있다.\n",
    "- `torch.full(shape, fill_value)`를 통해 `fill_value`로 가득한 텐서를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * torch.empty(2,2) : \n",
      "tensor([[1.4013e-45, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00]])\n",
      " * torch.zeros(3,2) : \n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      " * torch.ones(2,3) : \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      " * torch.full((3,3), 42) : \n",
      "tensor([[42, 42, 42],\n",
      "        [42, 42, 42],\n",
      "        [42, 42, 42]])\n"
     ]
    }
   ],
   "source": [
    "print(\" * torch.empty(2,2) : \", torch.empty(2, 2), sep='\\n')\n",
    "print(\" * torch.zeros(3,2) : \", torch.zeros(3 ,2), sep='\\n')\n",
    "print(\" * torch.ones(2,3) : \", torch.ones(2, 3), sep='\\n')\n",
    "print(\" * torch.full((3,3), 42) : \", torch.full((3,3), 42), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 언급한 `empty()`, `zeros()`, `ones()`, `full()`은 각각의 `_like`버전인 `empty_like()`, `zeros_like()`, `ones_like()`, `full_like()`가 있다.\n",
    "- 이 함수들은 텐서의 형태를 받는 대신 이미 초기화된 다른 텐서를 받아 그 텐서와 같은 형태이지만 값만 각각의 규칙으로 채운 텐서를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * torch.empty_like(sample_tensor) : \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      " * torch.zeros_like(sample_tensor) : \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      " * torch.ones_like(sample_tensor) : \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      " * torch.full_like(sample_tensor, 42) : \n",
      "tensor([[42., 42., 42.],\n",
      "        [42., 42., 42.],\n",
      "        [42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "sample_tensor = torch.eye(3)\n",
    "print(\" * torch.empty_like(sample_tensor) : \", torch.empty_like(sample_tensor), sep='\\n')\n",
    "print(\" * torch.zeros_like(sample_tensor) : \", torch.zeros_like(sample_tensor), sep='\\n')\n",
    "print(\" * torch.ones_like(sample_tensor) : \", torch.ones_like(sample_tensor), sep='\\n')\n",
    "print(\" * torch.full_like(sample_tensor, 42) : \", torch.full_like(sample_tensor, 42), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.asarray(obj)`를 통해 임의의 객체를 텐서로 전환할 수 있다.\n",
    "- 특히 `numpy.ndarray`와 같이 파이선의 버퍼 프로토콜을 구현하는 객체인 경우, 데이터를 복사하는 대신 `obj`의 데이터 메모리를 공유한다.\n",
    "- 이 외에도 유사한 함수로 `from_numpy`, `from_dlpack`, `frombuffer`, `as_tensor`, `as_strided` 등이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1:  tensor([1, 2, 3], dtype=torch.int32)\n",
      "shares memoery? :  True\n",
      "t2:  tensor([1., 2., 3.])\n",
      "shares memory? :  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TFG2576XG\\AppData\\Local\\Temp\\ipykernel_15984\\2748198648.py:14: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:998.)\n",
      "  torch.asarray(scalar)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 1.7500])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "array = numpy.array([1, 2, 3])\n",
    "# Shares memory with array 'array'\n",
    "t1 = torch.asarray(array)\n",
    "print(\"t1: \", t1)\n",
    "print(\"shares memoery? : \", array.__array_interface__['data'][0] == t1.data_ptr())\n",
    "# Copies memory due to dtype mismatch\n",
    "t2 = torch.asarray(array, dtype=torch.float32)\n",
    "print(\"t2: \", t2)\n",
    "print(\"shares memory? : \", array.__array_interface__['data'][0] == t1.data_ptr())\n",
    "\n",
    "scalar = numpy.float64(0.5)\n",
    "torch.asarray(scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.arange(start, end, step)`를 통해 $[\\text{start}, \\text{end})$ 사이의 수를 포함하는 벡터 텐서를 만들 수 있다.\n",
    "- 만들어지는 텐서의 길이는 다음 공식으로 구할 수 있다.\n",
    "$$\\Bigg\\lceil{\\frac{\\text{end} - \\text{start}}{step}}\\Bigg\\rceil$$\n",
    "- 예를 들어, `(0, 10, 2)`가 입력된 경우 길이는 $5$이다.\n",
    "  $$\\Bigg\\lceil{\\frac{\\text{10} - \\text{0}}{2}}\\Bigg\\rceil=5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4, 6, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원소 사이의 간격 `step`을 지정하여 프로그램이 원소의 개수를 정해주었던 `arange`와는 반대로,\n",
    "- `torch.linespace(start, end, steps)`는 원하는 원소의 개수를 미리 지정하여 프로그램이 `step`을 계산하도록 한다.\n",
    "- 원소 수열의 일반항은 다음과 같이 정의한다.\n",
    "  $$t : (a_0, a_1 \\cdots a_{steps-1})$$\n",
    "  $$a_i = \\text{start} + \\frac{(\\text{end}-\\text{start})}{\\text{steps}-1} * i$$\n",
    "- 예를 들어 `linespace(3,10,5)`인 경우 각 원소는 다음과 같다.\n",
    "  - $a_0 = 3 + 1.75 * 0 = 3$\n",
    "  - $a_1 = 3 + 1.75 * 1 = 4.75$\n",
    "  - $a_2 = 3 + 1.75 * 2 = 6.5$\n",
    "  - $a_3 = 3 + 1.75 * 3 = 8.25$\n",
    "  - $a_4 = 3 + 1.75 * 4 = 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(3, 10, steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.logspace(start, end, steps, base=10)`는 `linespace`와 사용법이 유사하다.\n",
    "- 다만, 각 원소를 계산할 때 지수의 밑(`base`)을 사용하며, 원소의 값을 계산하는 공식이 다르다.\n",
    "- `logspace`를 통해 생성된 텐서의 원소들은 로그 공간에서 간격이 균등하게 분포한다.\n",
    "- 즉, 인접한 두 원소의 지수의 차가 균일하다는 뜻이다.\n",
    "  $$t : (a_0, a_1 \\cdots a_{steps-1})$$\n",
    "  $$a_i = \\text{base}^{\\text{start} + \\frac{(\\text{end}-\\text{start})}{\\text{steps}-1} * i}$$\n",
    "- 위 일반항에서 지수부가 `linespace`의 $a_i$과 동일함을 확인할 수 있다\n",
    "- 예를 들어 `torch.logspace(-10, 10, 5)`인 경우 각 원소는 다음과 같다.\n",
    "  - $a_0 = 10^{-10 + 5 * 0} = 10^{-10}$\n",
    "  - $a_1 = 10^{-10 + 5 * 1} = 10^{-5}$\n",
    "  - $a_2 = 10^{-10 + 5 * 2} = 10^{0}$\n",
    "  - $a_3 = 10^{-10 + 5 * 3} = 10^{5}$\n",
    "  - $a_4 = 10^{-10 + 5 * 4} = 10^{10}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-10, 1.0000e-05, 1.0000e+00, 1.0000e+05, 1.0000e+10])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logspace(start=-10, end=10, steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.quantize_per_tensor()`, `torch.quantize_per_channel()`는 텐서의 모든 실수 원소를 정수 양자화(quantize) 한다.\n",
    "- 양자화된 텐서는 `torch.quint8`, `torch.qint8`, `torch.qint32` 세 타입 중 하나의 타입을 가진다.\n",
    "- `torch.dequantize()`는 양자화된 텐서를 32비트 플로트 텐서로 되돌린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)\n",
      "tensor([ 0, 10, 20, 30], dtype=torch.uint8)\n",
      "(tensor([-1.,  0.], size=(2,), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.10000000149011612,\n",
      "       zero_point=10), tensor([-2.,  2.], size=(2,), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.20000000298023224,\n",
      "       zero_point=20))\n",
      "tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.10000000149011612,\n",
      "       zero_point=10)\n",
      "================================================================================\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.1000, 0.0100], dtype=torch.float64),\n",
      "       zero_point=tensor([10,  0]), axis=0)\n",
      "tensor([[  0,  10],\n",
      "        [100, 200]], dtype=torch.uint8)\n",
      "================================================================================\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8))\n",
    "print(torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr())\n",
    "print(torch.quantize_per_tensor([torch.tensor([-1.0, 0.0]), torch.tensor([-2.0, 2.0])], torch.tensor([0.1, 0.2]), torch.tensor([10, 20]), torch.quint8))\n",
    "print(torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), torch.tensor(0.1), torch.tensor(10), torch.quint8))\n",
    "print(\"=\"*80)\n",
    "x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])\n",
    "print(torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8))\n",
    "print(torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr())\n",
    "print(\"=\"*80)\n",
    "print(torch.dequantize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.complex(real, imag)`는 실수부를 나타내는 텐서와 허수부를 나타내는 텐서를 조합하여 복소수 텐서를 만든다.\n",
    "- 두 텐서의 형태는 서로 같아야 한다.\n",
    "- `torch.is_complex(tensor)`는 텐서가 복소수 형식인가를 확인한다.\n",
    "- `torch.imag(tesnor)`는 텐서의 허수부를 분리해낸다.\n",
    "- `torch.real(tensor)`은 텐서의 실수부를 분리해낸다.\n",
    "- `torch.abs(tensor)`는 텐서의 절댓값을 구하는 함수인데, 복소수의 절댓값은 복소수 벡터의 길이이다. $z = a+bi→ |z| = \\sqrt{a^2+b^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z :  tensor([1.+2.j, 3.+4.j])\n",
      "torch.is_complex(z) :  True\n",
      "torch.imag(z) :  tensor([2., 4.])\n",
      "torch.real(z) :  tensor([1., 3.])\n",
      "torch.abs(z) :  tensor([2.2361, 5.0000])\n"
     ]
    }
   ],
   "source": [
    "real = torch.tensor([1, 3], dtype=torch.float32)\n",
    "imag = torch.tensor([2, 4], dtype=torch.float32)\n",
    "z = torch.complex(real, imag)\n",
    "print(\"z : \", z)\n",
    "print(\"torch.is_complex(z) : \", torch.is_complex(z))\n",
    "print(\"torch.imag(z) : \", torch.imag(z))\n",
    "print(\"torch.real(z) : \", torch.real(z))\n",
    "print(\"torch.abs(z) : \", torch.abs(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.polar(abs, angle)`는 극분해(polar decomposition)의 역연산이다. 즉, 복소수의 절댓값($abs$ 혹은 $|z|$)와 편각($\\theta$)을 받아서 복소수 $z$를 구한다.\n",
    "- `abs`는 음수이거나 `NaN`일 수 없다.\n",
    "- `angle`은 무한일 수 없다.\n",
    "- $\\theta$만큼 회전하는 회전행렬의 형태는 다음과 같다:\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  \\cos{\\theta} & -\\sin{\\theta}\\\\\n",
    "  \\sin{\\theta} & \\cos{\\theta}\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "- 복소수의 행렬 표현은 다음과 같다:\n",
    "  $$\n",
    "  z = a + bi = \\begin{bmatrix}\n",
    "  a & -b\\\\\n",
    "  b & a\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "- 두 표현 사이에 유사성이 있으므로, 다음과 같이 나타낼 수 있다\n",
    "  $$\n",
    "  \\text{let}\\quad |z| = \\sqrt{a^2+b^2}, \\quad a = |z|\\sin{\\theta}, \\quad b = |z|\\cos{\\theta}\\\\[2em]\n",
    "  \\text{then, }\\quad z = a + bi = \\begin{bmatrix}\n",
    "  a & -b\\\\\n",
    "  b & a\n",
    "  \\end{bmatrix}\n",
    "  =\n",
    "  \\begin{bmatrix}\n",
    "  |z|\\sin{\\theta} & -|z|\\cos{\\theta}\\\\\n",
    "  |z|\\cos{\\theta} & |z|\\sin{\\theta}\n",
    "  \\end{bmatrix}\n",
    "  = |z|\\begin{bmatrix}\n",
    "  \\cos{\\theta} & -\\sin{\\theta}\\\\\n",
    "  \\sin{\\theta} & \\cos{\\theta}\n",
    "  \\end{bmatrix}\n",
    "  = |z|(\\cos{\\theta} + i\\sin{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.1232e-17+1.0000j, -1.4142e+00-1.4142j], dtype=torch.complex128)\n",
      "tensor([1., 2.], dtype=torch.float64)\n",
      "tensor([ 6.1232e-17, -1.4142e+00], dtype=torch.float64)\n",
      "tensor([ 1.0000, -1.4142], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "abs = torch.tensor([1, 2], dtype=torch.float64)\n",
    "angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)\n",
    "z = torch.polar(abs, angle)\n",
    "print(z)\n",
    "print(z.abs())\n",
    "print(torch.real(z)) # 0, -1.4142 = -sqrt(2)\n",
    "print(torch.imag(z)) # 1, -1.4142 = -sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.adjoint`는 복소수 행렬을 입력받아 켤레 전치를 구한다.\n",
    "- 켤레 전치는 adjoint 혹은 conjugate transpose 혹은 Hermitian transpose라고도 불린다.\n",
    "- 예를 들어 $A = \\begin{bmatrix}\n",
    "1 + i & 2 + 2i \\\\\n",
    "3 + 3i & 4 +4i\n",
    "\\end{bmatrix}$라면 켤레 전치 $A^\\dagger = \\begin{bmatrix}\n",
    "1 - i & 3 - 3i \\\\\n",
    "2 - 2i & 4 -4i\n",
    "\\end{bmatrix}$이며,\n",
    "- $A^\\dagger$는 \"에이 대거\"라 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.+0.j, 1.+1.j],\n",
      "        [2.+2.j, 3.+3.j]])\n",
      "tensor([[0.-0.j, 2.-2.j],\n",
      "        [1.-1.j, 3.-3.j]])\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float)\n",
    "A = torch.complex(x, x).reshape(2, 2)\n",
    "print(A)\n",
    "print(A.adjoint())\n",
    "print(A.adjoint() == A.mH.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.heaviside(input, values, *, out=None)`는 텐서에 단위 계단 함수(Heaviside step function)를 적용한다.\n",
    "$$\n",
    "\\text{heaviside}(input, values) = \\begin{cases}\n",
    "0&(\\text{if }input < 0)\\\\\n",
    "values&(\\text{if }input = 0)\\\\\n",
    "1&(\\text{if }input > 0)\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5000,  0.0000,  2.0000])\n",
      "========================================\n",
      "values :  tensor([0.5000])\n",
      "heaviside :  tensor([0.0000, 0.5000, 1.0000])\n",
      "========================================\n",
      "values :  tensor([ 1.2000, -2.0000,  3.5000])\n",
      "heaviside :  tensor([ 0., -2.,  1.])\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([-1.5, 0, 2.0])\n",
    "print(input)\n",
    "print(\"=\"*40)\n",
    "values = torch.tensor([0.5])\n",
    "print(\"values : \", values)\n",
    "print(\"heaviside : \", torch.heaviside(input, values))\n",
    "print(\"=\"*40)\n",
    "values = torch.tensor([1.2, -2.0, 3.5])\n",
    "print(\"values : \", values)\n",
    "print(\"heaviside : \", torch.heaviside(input, values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.reshape(tensor, shape)`는 텐서의 형태를 주어진 `shape`로 변경한다.\n",
    "- 어떤 텐서를 쪼개어 더 높은 차원의 텐서를 만드는 것에 유용하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : \n",
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
      "torch.reshape(a, (4, 4)) : \n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]])\n",
      "torch.reshape(a, (2, 2, 4)) : \n",
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8]],\n",
      "\n",
      "        [[ 9, 10, 11, 12],\n",
      "         [13, 14, 15, 16]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(1, 17, 1)\n",
    "print(\"a : \", a, sep='\\n')\n",
    "print(\"torch.reshape(a, (4, 4)) : \", torch.reshape(a, (4, 4)), sep='\\n')\n",
    "print(\"torch.reshape(a, (2, 2, 4)) : \", torch.reshape(a, (2, 2, 4)), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `shape=(-1,)`로 지정할 경우 반대로 높은 차원의 텐서의 원소를 풀어내어 1차원으로 만들 수 있다.\n",
    "- `shape`에 의해 새로 지정되는 형태가 원래 텐서의 모든 원소를 담을 수 있어야 한다.\n",
    "- 예를 들어 다음 코드의 텐서 `b`는 9개의 원소가 있으므로, `shape`의 값으로 `(1, 9)`, `(3, 3)`, `(9, 1)` 정도를 쓸 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * b : \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      " * torch.reshape(b, (-1,)) : \n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      " * torch.reshape(b, (1,9)) : \n",
      "tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      " * torch.reshape(b, (3,3)) : \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      " * torch.reshape(b, (9,1)) : \n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [7],\n",
      "        [8],\n",
      "        [9]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(\" * b : \", b, sep='\\n')\n",
    "print(\" * torch.reshape(b, (-1,)) : \", torch.reshape(b, (-1,)), sep='\\n')\n",
    "print(\" * torch.reshape(b, (1,9)) : \", torch.reshape(b, (1,9)), sep='\\n')\n",
    "print(\" * torch.reshape(b, (3,3)) : \", torch.reshape(b, (3,3)), sep='\\n')\n",
    "print(\" * torch.reshape(b, (9,1)) : \", torch.reshape(b, (9,1)), sep='\\n')\n",
    "# print(\"torch.reshape(b, (9,1)) : \", torch.reshape(b, (4,3)), sep='\\n') # RuntimeError: shape '[4, 3]' is invalid for input of size 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.argwhere()` 함수를 통해 텐서 속에서 `0`이 아닌 원소의 인덱스 목록을 담은 텐서를 반환한다.\n",
    "- 희소행렬에서 어디에 원소가 있는지 확인하는데 사용할 수 있겠다.\n",
    "- 인덱스는 사전 배열 순서로 정렬된다.\n",
    "- 예를 들어 다음 행렬은 1행 1열, 1행 3열, 2행 2열, 2행 3열의 인덱스가 순서대로 담긴다\n",
    "- $\\begin{bmatrix}1&0&2\\\\0&3&4\\end{bmatrix}$ → `[[0, 0],[0, 2],[1, 1],[1, 2]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [2]])\n",
      "tensor([[0, 0],\n",
      "        [0, 2],\n",
      "        [1, 1],\n",
      "        [1, 2]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([1, 0, 1])\n",
    "print(torch.argwhere(t))\n",
    "t = torch.tensor([[1, 0, 1], [0, 1, 1]])\n",
    "print(torch.argwhere(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.where(condition, input, other, *, out=None)`는 원래 텐서의 각 스칼라에 필터를 적용하여 변환된 텐서를 반환한다.\n",
    "- 따라서 `where`가 반환하는 텐서는 원래 텐서와 형태가 같다.\n",
    "- `where`가 반환하는 텐서의 각 값은 `condition` 함수가 `True`를 반환한 경우 `input` 값, `False`인 경우 `other` 값이 적힌다.\n",
    "- `input`, `other`는 하나의 스칼라 값 대신에 검사되는 텐서와 같은 형태의 텐서를 넣을 수도 있다. 그럴 경우 검사되는 원소와 같은 위치의 값이 선택된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "========================================\n",
      "torch.where(x % 2 == 0, 1.0, 0.0) : \n",
      "tensor([[0., 1., 0.],\n",
      "        [1., 0., 1.]])\n",
      "========================================\n",
      "torch.where(x % 2 == 0, x, y : \n",
      "tensor([[-1,  2, -3],\n",
      "        [ 4, -5,  6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "y = torch.tensor([\n",
    "    [-1, -2, -3],\n",
    "    [-4, -5, -6]\n",
    "])\n",
    "print(\"x : \")\n",
    "print(x)\n",
    "print(\"=\"*40)\n",
    "print(\"torch.where(x % 2 == 0, 1.0, 0.0) : \")\n",
    "print(torch.where(x % 2 == 0, 1.0, 0.0))\n",
    "print(\"=\"*40)\n",
    "print(\"torch.where(x % 2 == 0, x, y : \")\n",
    "print(torch.where(x % 2 == 0, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.cat()` 혹은 `torch.concat()` 혹은 `torch.concatenate()`은 입력 받은 두 텐서의 원소를 모두 포함한 새로운 텐서를 만든다.\n",
    "- 입력 텐서는 모두 형태(`shape`)가 같아야 한다.\n",
    "- `dim`을 통해 출력될 텐서의 차원을 지정할 수 있다.\n",
    "- `dim=0` 혹은 `dim=-1`일 경우 최상위 랭크에서 텐서를 결합한다.\n",
    "  - 아래 예시에서 행렬의 최상위 랭크는 행 벡터의 배열이므로, 행 배열들이 차곡차곡 쌓여 세로로 길어진다.\n",
    "- `dim=1` 혹은 `dim=-2`일 경우 최상위 랭크 바로 아래에서 텐서를 결합한다.\n",
    "  - 아래 예시에서 행렬의 아래 랭크는 원소의 배열이므로, 원소들이 결합되어 가로로 길어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * x : \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      " * y : \n",
      "tensor([[ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      " * torch.cat((x,y), dim=0)) : \n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      " * torch.cat((x,y), dim=1)) : \n",
      "tensor([[ 1,  2,  3,  7,  8,  9],\n",
      "        [ 4,  5,  6, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 7).reshape(2,3)\n",
    "y = torch.arange(7, 13).reshape(2,3)\n",
    "print(\" * x : \", x, sep='\\n')\n",
    "print(\" * y : \", y, sep='\\n')\n",
    "print(\" * torch.cat((x,y), dim=0)) : \", torch.cat((x, y), dim=0), sep='\\n')\n",
    "print(\" * torch.cat((x,y), dim=1)) : \", torch.cat((x, y), dim=1), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 켤레 복소수 행렬은 특별히 느린 연산 패턴(lazy evaluation)을 지원한다.\n",
    "- 텐서의 켤레 비트(conjugate bit)를 `1`로 설정하면, 이 텐서가 나중에 연산에 사용되기 직전까지 실제로 원소들을 켤레 복소수로 변환하는 것을 미룬다.\n",
    "- `torch.conj(tensor)` 함수로 텐서의 켤레 비트를 `1`로 설정할 수 있다.\n",
    "- `torch.is_conj(tensor)` 함수를 통해 텐서의 켤레 비트가 설정되어 있는지 확인할 수 있다.\n",
    "- `torch.resolve_conj(tensor)` 함수로 전달받은 텐서의 켤레 비트가 설정되어 있는 경우 켤레 복소수를 담고 있는 새로운 텐서를 생산할 수 있다.\n",
    "- (현재 torch 버전 1.2)미래에는 `torch.conj(tensor)`가 느린 연산 패턴을 하는 것이 아니라 뷰 오브젝트를 반환할 예정이라고 한다. 이것은 코드의 형태에 크게 영향을 미치므로, 이 행동을 그대로 가져가고자 한다면 `torch.conj_physical()`를 대신 사용하는 것이 좋다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.+1.j, -2.+2.j,  3.-3.j]) False\n",
      "tensor([-1.-1.j, -2.-2.j,  3.+3.j]) True\n",
      "tensor([-1.-1.j, -2.-2.j,  3.+3.j]) False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j])\n",
    "print(x, torch.is_conj(x))\n",
    "y = torch.conj(x)\n",
    "print(y, torch.is_conj(y))\n",
    "z = torch.resolve_conj(y)\n",
    "print(z, torch.is_conj(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.chunk(tensor, chunks, dim)`는 텐서를 여러 덩어리(chunk)로 나눈다.\n",
    "- `chunks`는 얻고자 하는 덩어리의 개수를 지정한다.\n",
    "- 덩어리로 나눠진 텐서는 실제 스토리지를 가진게 아니라 원래 텐서의 뷰 오브젝트이다.\n",
    "- 만약 나누고자 차원의 텐서 수가 청크 개수로 나누어 떨어진다면 똑같은 형태의 청크가 여럿 생길 것이다.\n",
    "  - 만약 나누어 떨어지지 않는다면 마지막 청크만 조금 작을 것이다.\n",
    "  - 균등하게 나누려고 하였을 때, 목표한 청크 개수에 도달하지 못할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1]), tensor([2, 3]), tensor([4, 5]), tensor([6, 7]), tensor([8, 9]), tensor([10]))\n",
      "(tensor([0, 1]), tensor([2, 3]), tensor([4, 5]), tensor([6, 7]), tensor([8, 9]), tensor([10, 11]))\n",
      "(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([ 9, 10, 11]), tensor([12]))\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(11).chunk(6)) # 균등하게 나누려 하다보니 마지막 덩어리만 하나 적다.\n",
    "print(torch.arange(12).chunk(6)) # 균등하게 나누니 2개씩 나누어 떨어졌다.\n",
    "print(torch.arange(13).chunk(6)) # 균등하게 나누려 하다보니 덩어리가 5개만 생긴다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.column_stack(tensors)`는 텐서의 튜플을 받아, 튜플 내의 각 텐서를 열 벡터로 보고 가로로(오른쪽으로) 차곡차곡 쌓아서 행렬을 만든다.\n",
    "  - 입력된 텐서가 1차원 벡터인 경우, 열 행렬로 변환한 뒤 쌓는다.\n",
    "    - a=$\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}$\n",
    "    - b=$\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix}$\n",
    "    - ab = $\\begin{bmatrix}1&4\\\\2&5\\\\3&6\\end{bmatrix}$\n",
    "  - 입력된 텐서가 2차원 행렬인 경우, 그대로 가져다 쌓는다.\n",
    "    - abab = $\\begin{bmatrix}1&4&1&4\\\\2&5&2&5\\\\3&6&3&6\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: \n",
      "tensor([1, 2, 3])\n",
      "b: \n",
      "tensor([4, 5, 6])\n",
      "ab: \n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "abab: \n",
      "tensor([[1, 4, 1, 4],\n",
      "        [2, 5, 2, 5],\n",
      "        [3, 6, 3, 6]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "print(\"a: \", a, sep='\\n')\n",
    "b = torch.tensor([4, 5, 6])\n",
    "print(\"b: \", b, sep='\\n')\n",
    "ab = torch.column_stack((a, b))\n",
    "print(\"ab: \", ab, sep='\\n')\n",
    "abab = torch.column_stack((ab, ab))\n",
    "print(\"abab: \", abab, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.hstack()`또한 `column_stack()`처럼 텐서를 가로로 쌓지만, 벡터 텐서를 굳이 열벡터로 변환하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "print(torch.hstack((a,b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.hsplit(input, indices_or_sections)`은 텐서를 가로로 쪼개어 원래 텐서의 뷰를 생성한다.\n",
    "- `indices_or_sections`이 스칼라 값인 경우, 섹션의 개수를 뜻한다.\n",
    "  - 예를 들어 다음 코드는 4개의 열을 가진 행렬을 2개의 열을 가진 2개의 행렬로 쪼개고 있다.\n",
    "- `indices_or_sections`가 1차원 배열인 경우, 쪼개고자 하는 위치의 인덱스를 의미한다.\n",
    "  - 예를 들어 다음 코드는 4개의 열을 가진 행렬을 3번째 열을 기준으로 두 개의 행렬로 쪼개고 있다.\n",
    "    - 3번째 열 이전(`[:2, :]`)\n",
    "    - 3번째 열 부터 나머지(`[2:, :]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t :  tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.]])\n",
      "========================================\n",
      "(tensor([[ 0.,  1.],\n",
      "        [ 4.,  5.],\n",
      "        [ 8.,  9.],\n",
      "        [12., 13.]]), tensor([[ 2.,  3.],\n",
      "        [ 6.,  7.],\n",
      "        [10., 11.],\n",
      "        [14., 15.]]))\n",
      "========================================\n",
      "(tensor([[ 0.,  1.],\n",
      "        [ 4.,  5.],\n",
      "        [ 8.,  9.],\n",
      "        [12., 13.]]), tensor([[ 2.,  3.],\n",
      "        [ 6.,  7.],\n",
      "        [10., 11.],\n",
      "        [14., 15.]]))\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(16.0).reshape(4,4)\n",
    "print(\"t : \", t)\n",
    "print(\"=\"*40)\n",
    "print(torch.hsplit(t, 2))\n",
    "print(\"=\"*40)\n",
    "print(torch.hsplit(t, [2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.row_stack()` 혹은 `torch.vstack()`은 텐서의 튜플을 입력받아, 튜플 내의 각 텐서를 세로로(아래 방향으로) 차곡차곡 쌓아 행렬을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "print(torch.vstack((a,b)))\n",
    "print(torch.row_stack((a,b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.vsplit(input, indices_or_sections)`은 텐서를 세로로 쪼개어 원래 텐서의 뷰를 생성한다.\n",
    "- `indices_or_sections`이 스칼라 값인 경우, 섹션의 개수를 뜻한다.\n",
    "  - 예를 들어 다음 코드는 4개의 행을 가진 행렬을 2개의 행을 가진 2개의 행렬로 쪼개고 있다.\n",
    "- `indices_or_sections`가 1차원 배열인 경우, 쪼개고자 하는 위치의 인덱스를 의미한다.\n",
    "  - 예를 들어 다음 코드는 4개의 행을 가진 행렬을 3번째 행을 기준으로 두 개의 행렬로 쪼개고 있다.\n",
    "    - 3번째 행 이전(`[:, 0:2]`)\n",
    "    - 3번째 행 부터 나머지(`[:, 2:]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t :  tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.]])\n",
      "========================================\n",
      "torch.vsplit(t, 2) : \n",
      "(tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]]), tensor([[ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.]]))\n",
      "========================================\n",
      "torch.vsplit(t, [2]) : \n",
      "(tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]]), tensor([[ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.]]))\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(16.0).reshape(4,4)\n",
    "print(\"t : \", t)\n",
    "split = torch.vsplit(t, 2)\n",
    "print(\"=\"*40)\n",
    "print(\"torch.vsplit(t, 2) : \", torch.vsplit(t, 2), sep='\\n')\n",
    "print(\"=\"*40)\n",
    "print(\"torch.vsplit(t, [2]) : \")\n",
    "print(torch.vsplit(t, [2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.transpose(tensor, dim0, dim1)` 혹은 `swapaxes`, `swapdims`는 전치 텐서를 구한다.\n",
    "- 일반적으로 전치(transpose)란 2차원 행렬에 적용되는 연산이며, 행과 열을 뒤집는 것이다.\n",
    "- 어떤 행렬 $A$가 있을 때 $A$의 전치 행렬은 기호로 $A^T$로 나타낸다.\n",
    "$$\n",
    "A = \\begin{bmatrix}1&2&3\\\\4&5&6\\end{bmatrix},\\quad\n",
    "A^T = \\begin{bmatrix}1&4\\\\2&5\\\\3&6\\end{bmatrix}\n",
    "$$\n",
    "- 다른 방식으로는, 두 행렬의 원소의 인덱스가 뒤집혔음을 암시하여 $M_{ij}$, $M_{ji}$로 나타내기도 한다.\n",
    "$$\n",
    "{M_{ij}}^T = M_{ji}\n",
    "$$\n",
    "- 이차원 행렬의 전치를 구하려면 `dim0=0`, `dim1=1`로 설정한다.\n",
    "- 이차원 행렬의 전치의 경우, 어짜피 교환할 수 있는 차원이 단 두개 뿐이므로, `torch.t(tensor)`로 줄여 쓸 수 있다.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A : \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.transpose(A, 0, 1) : \n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.t(A) : \n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(1, 7).reshape(2,3)\n",
    "print(\"A : \", x, sep='\\n')\n",
    "print(\"torch.transpose(A, 0, 1) : \", torch.transpose(A, 0, 1), sep='\\n')\n",
    "print(\"torch.t(A) : \" , torch.t(A), sep='\\n')\n",
    "print(torch.t(A) == torch.transpose(A, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 전치 연산은 행렬 뿐만 아니라, 더 고차원의 텐서에 대해서도 정의된다.\n",
    "- 예를 들어 3차원 텐서의 전치는 `xy`를 교환한 것, `yz`를 교환한 것, `zx`를 교환한 것 세 가지가 나오겠다. 각각의 경우 `dim0`, `dim1`의 값은 다음과 같다.\n",
    "  |축 이름|번호|바꾸고자 하는 축|dim0|dim1|\n",
    "  |-|-|-|-|-|\n",
    "  |x|2|xy|2|1|\n",
    "  |y|1|yz|1|0|\n",
    "  |z|0|zx|0|2|\n",
    "- 예를 들어 3차원 텐서 $B=\\Bigg(\\begin{bmatrix}1&2\\\\3&4\\end{bmatrix},\\begin{bmatrix}5&6\\\\7&8\\end{bmatrix}\\Bigg)$에서 두 차원을 서로 교환하여 얻어지는 전치는 다음과 같이 세 가지가 있다.\n",
    "  - xy 전치($M_{ijk} → M_{jik}$) : $\\Bigg(\\begin{bmatrix}1&3\\\\2&4\\end{bmatrix},\\begin{bmatrix}5&7\\\\6&8\\end{bmatrix}\\Bigg)$. \n",
    "    - `y[i,j,k]`에 있던 원소가 `[i,k,j]`로 옮겨갔다.\n",
    "    - 따라서 모든 원소의 z축 인덱스는 변하지 않았다.\n",
    "  - yz 전치($M_{ijk} → M_{ikj}$) : $\\Bigg(\\begin{bmatrix}1&2\\\\5&6\\end{bmatrix},\\begin{bmatrix}3&4\\\\7&8\\end{bmatrix}\\Bigg)$.\n",
    "    - `y[i,j,k]`에 있던 원소가 `[j,i,k]`로 옮겨갔다.\n",
    "    - 따라서 모든 원소의 x축 인덱스는 변하지 않았다.\n",
    "  - zx 전치($M_{ijk} → M_{kji}$) : $\\Bigg(\\begin{bmatrix}1&5\\\\3&7\\end{bmatrix},\\begin{bmatrix}2&6\\\\4&8\\end{bmatrix}\\Bigg)$.\n",
    "    - `y[i,j,k]`에 있던 원소가 `[k,j,i]`로 옮겨갔다.\n",
    "    - 따라서 모든 원소의 y축 인덱스는 변하지 않았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B : \n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "B = torch.arange(1, 9).reshape(2,2,2)\n",
    "print(\"B : \", B, sep='\\n')\n",
    "Bxy = torch.transpose(B, 2, 1)\n",
    "Byz = torch.transpose(B, 1, 0)\n",
    "Bzx = torch.transpose(B, 0, 2)\n",
    "for i in range(0, 2):\n",
    "    for j in range(0, 2):\n",
    "        for k in range(0, 2):\n",
    "            assert(B[i, j, k] == Bxy[i, k, j])\n",
    "            assert(B[i, j, k] == Byz[j, i, k])\n",
    "            assert(B[i, j, k] == Bzx[k, j, i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.movedim(tensor, source, destination)` 혹은 `torch.moveaxis()`는 어떤 텐서의 차원을 `source, destination` 간에 변환한다.\n",
    "- `source`, `destination`에 각각 `int`를 넣는 경우, 두 차원을 서로 교환한다. 이는 `torch.transpose`를 부르는 것과 같다.\n",
    "- `source`, `destination`에 각각 `sizes`를 넣는 경우, 둘 이상의 차원을 서로 교환한다. 이는 `torch.transpose`를 연속해서 여러번 부르는 것과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]])\n",
      "\n",
      "torch.Size([1, 2, 3])\n",
      "tensor([[[True, True, True],\n",
      "         [True, True, True]]])\n",
      "\n",
      "torch.Size([1, 3, 2])\n",
      "tensor([[[1, 4],\n",
      "         [2, 5],\n",
      "         [3, 6]]])\n",
      "tensor([[[True, True],\n",
      "         [True, True],\n",
      "         [True, True]]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(1,7).reshape(2,1,3)\n",
    "print(t)\n",
    "print()\n",
    "print(torch.movedim(t, 1, 0).shape)\n",
    "print(torch.movedim(t, 1, 0) == torch.transpose(t, 1, 0))\n",
    "print()\n",
    "print(torch.movedim(t, (1, 2), (0, 1)).shape)\n",
    "print(torch.movedim(t, (1, 2), (0, 1)))\n",
    "print(torch.movedim(t, (1, 2), (0, 1)) == torch.transpose(torch.transpose(t, 1, 0), 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.permute(input, dims)`또한 `transpose`, `movedim`과 같이 차원을 교환하는 함수이다.\n",
    "- 다만, 교환하려는 두 차원을 각각 지정했던 `transpose(dim0, dim1)`나 `movedim(source, destination)`과는 달리,\n",
    "- `permute`는 모든 차원을 한 번 씩 옮김을 가정하고, 목적지만을 `dims`에 튜플로 제시한다.\n",
    "- 어떤 차원을 반복해서 사용할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B : \n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "B = torch.arange(1, 9).reshape(2,2,2)\n",
    "print(\"B : \", B, sep='\\n')\n",
    "# Biii = torch.permute(B, (0,0,0)) # RuntimeError: repeated dim in permute\n",
    "Bijk = torch.permute(B, (0,1,2))\n",
    "Bikj = torch.permute(B, (0,2,1))\n",
    "Bjik = torch.permute(B, (1,0,2))\n",
    "Bjki = torch.permute(B, (1,2,0))\n",
    "Bkij = torch.permute(B, (2,0,1))\n",
    "Bkji = torch.permute(B, (2,1,0))\n",
    "\n",
    "for i in range(0, 2):\n",
    "    for j in range(0, 2):\n",
    "        for k in range(0, 2):\n",
    "            assert(B[i, j, k] == Bijk[i, j, k])\n",
    "            assert(B[i, j, k] == Bikj[i, k, j])\n",
    "            assert(B[i, j, k] == Bjik[j, i, k])\n",
    "            assert(B[i, j, k] == Bjki[j, k, i])\n",
    "            assert(B[i, j, k] == Bkij[k, i, j])\n",
    "            assert(B[i, j, k] == Bkji[k, j, i])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.narrow(input, dim, start, length)`는 텐서의 한 차원에서 테두리를 깎아내고 일부만 남겨 더 '좁은' 텐서를 만든다.\n",
    "- `dim` 차원에서 $[start, start + length)$범위에 있는 아이템만 남기고, 나머지 아이템을 버린다.\n",
    "- `dim`, `start`는 인덱싱을 할 때와 동일하게 음수 인덱스를 지정할 수 있다.\n",
    "- 이 작업은 `[]` 슬라이스로도 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[2, 3],\n",
      "        [5, 6],\n",
      "        [8, 9]])\n",
      "tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "tensor([[3],\n",
      "        [6],\n",
      "        [9]])\n",
      "tensor([[True],\n",
      "        [True],\n",
      "        [True]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(torch.narrow(x, 0, 0, 2))\n",
    "print(torch.narrow(x, 0, 0, 2) == x[0:2,:])\n",
    "print(torch.narrow(x, 1, 1, 2))\n",
    "print(torch.narrow(x, 1, 1, 2) == x[:,1:3])\n",
    "print(torch.narrow(x, -1, -1, 1))\n",
    "print(torch.narrow(x, -1, -1, 1) == x[:,-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.select(input, dim, index)`는 특정 차원 `dim`에서 `index` 부분만 잘라내어 새로운 텐서를 만든다.\n",
    "- 이 작업은 `[]` 슬라이스로도 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6],\n",
      "        [7, 8]])\n",
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[2, 4],\n",
      "        [6, 8]])\n",
      "tensor([[True, True],\n",
      "        [True, True]])\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "print(torch.select(B, 0, index))\n",
    "print(torch.select(B, 0, index) == B[index]) # i 인덱스가 `index`인 것만 모두 골라내겠습니다.\n",
    "print(torch.select(B, 2, index))\n",
    "print(torch.select(B, 2, index) == B[:,:,index]) # k 인덱스가 `index`인 것만 모두 골라내겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.stack(tensors, dim=0, *, out=None)`는 텐서의 시퀀스를 새 차원에 이어붙이되, 지정한 차원의 값들 끼리 이어 붙인다.\n",
    "- 모든 텐서는 형태가 서로 같아야 한다.\n",
    "- 새 차원에 평행하게 붙인다는 말은, 반환되는 새로운 텐서는 입력된 텐서보다 한 차원 높은 텐서가 된다는 뜻이다.\n",
    "- 예를 들어 2차원 행렬 두 개를 서로 붙이면, 3차원의 길이가 2인 텐서가 만들어진다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[5, 6],\n",
      "        [7, 8]])\n",
      "========================================\n",
      "torch.Size([2, 2, 2])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "========================================\n",
      "torch.Size([2, 2, 2])\n",
      "tensor([[[1, 2],\n",
      "         [5, 6]],\n",
      "\n",
      "        [[3, 4],\n",
      "         [7, 8]]])\n",
      "========================================\n",
      "torch.Size([2, 2, 2])\n",
      "tensor([[[1, 5],\n",
      "         [2, 6]],\n",
      "\n",
      "        [[3, 7],\n",
      "         [4, 8]]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(1, 5).reshape(2,2)\n",
    "Y = torch.arange(5, 9).reshape(2,2)\n",
    "print(X)\n",
    "print(Y)\n",
    "print(\"=\"*40)\n",
    "print(torch.stack((X, Y), 0).shape, torch.stack((X, Y), 0), sep='\\n')\n",
    "print(\"=\"*40)\n",
    "print(torch.stack((X, Y), 1).shape, torch.stack((X, Y), 1), sep='\\n')\n",
    "print(\"=\"*40)\n",
    "print(torch.stack((X, Y), 2).shape, torch.stack((X, Y), 2), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.unbind(tensor, dim=0)`는 입력된 텐서의 지정된 차원을 풀어헤쳐서, 여러 개의 텐서를 만든다.\n",
    "- 따라서 `torch.stack`의 정반대되는 기능이라 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unbind(torch.tensor([[1, 2, 3],\n",
    "                           [4, 5, 6],\n",
    "                           [7, 8, 9]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.tile(input, dims)`로 어떤 패턴을 타일처럼 반복하여 더 큰 텐서를 만들 수 있다.\n",
    "- `dims`는 각 차원 방향으로 반복하고자 하는 회수를 지정한다.\n",
    "- 예를 들어 `dims=(2,2)`일 경우 x축 방향으로 2개, y축 방향으로 2개로 총 4 개의 타일이 형성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 1, 2, 3])\n",
      "tensor([[1, 2, 1, 2],\n",
      "        [3, 4, 3, 4],\n",
      "        [1, 2, 1, 2],\n",
      "        [3, 4, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "print(x.tile((2,)))\n",
    "y = torch.tensor([[1, 2], [3, 4]])\n",
    "print(torch.tile(y, (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.gather(tensor, dim, index)`는 주어진 텐서에서 값을 임의로 골라내어 새로운 텐서를 만드는 함수로서,\n",
    "- 단순한 인덱싱과 슬라이싱 만으로는 표현하기 힘든 복잡한 값 추출을 단순하게 표현할 수 있다.\n",
    "- `dim`은 `index`를 적용하고자 하는 차원을 지정한다.\n",
    "  - `dim=0`이면 `t[index[i, j], j]`를 구한다. 즉, 열은 같은 위치의 열을 보되, 값을 가져오는 행의 위치가 바뀐다.\n",
    "  - `dim=1`이면 `t[i, index[i, j]]`을 구한다. 즉, 행은 같은 위치의 행을 보되, 값을 가져오는 열의 위치가 바뀐다.\n",
    "- `index`는 추출하고자 하는 값의 인덱스를 담은 텐서이다.\n",
    "- `index` 텐서의 형태가 곧 반환되는 결과 텐서의 형태이다.\n",
    "- 이떄 `index`의 원소로는 음수 인덱스를 쓸 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t : \n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]])\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 1,  2,  3,  8],\n",
      "        [ 1,  2,  3, 12],\n",
      "        [ 1,  2,  3, 16],\n",
      "        [ 1,  6, 11, 16],\n",
      "        [ 1,  2,  7,  8],\n",
      "        [13, 14, 15, 16]])\n",
      "tensor([[ 1,  1,  1,  1,  1,  1,  4],\n",
      "        [ 5,  5,  5,  5,  6,  5,  8],\n",
      "        [ 9,  9,  9,  9, 11, 10, 12],\n",
      "        [13, 14, 15, 16, 16, 14, 16]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10 ,11, 12],\n",
    "    [13, 14, 15, 16]\n",
    "])\n",
    "print(\"t : \", t, sep='\\n')\n",
    "print(torch.gather(t, 0, torch.tensor([\n",
    "    [0, 0, 0, 0 ], \n",
    "    [0, 0, 0, 1 ],\n",
    "    [0, 0, 0, 2 ],\n",
    "    [0, 0, 0, 3 ],\n",
    "    [0, 1, 2, 3 ],\n",
    "    [0, 0, 1, 1 ],\n",
    "    [3, 3, 3, 3 ]])))\n",
    "print(torch.gather(t, 1, torch.tensor([\n",
    "    [0, 0, 0, 0, 0, 0, 3 ], \n",
    "    [0, 0, 0, 0, 1, 0, 3 ],\n",
    "    [0, 0, 0, 0, 2, 1, 3 ],\n",
    "    [0, 1, 2, 3, 3, 1, 3]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.scatter(out, dim, index, src)`는 `gatehr`의 반대 되는 작업으로서,\n",
    "- `src` 텐서의 값을 `dim`과 `index`를 참조하여 `out` 텐서에 복사해 넣는다.\n",
    "- `out`, `index`, `src` 셋 모두 텐서이다.\n",
    "- `scatter`는 `src`의 모든 원소를 한 번씩 보듯 루프를 돌며 `out`의 `index` 위치에 값을 집어넣는다.\n",
    "- `dim`에 따라 `index`가 쓰이는 차원이 정해진다.\n",
    "  ```py\n",
    "  self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\n",
    "  self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\n",
    "  self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n",
    "  ```\n",
    "- 따라서 `index`, `src`, `out`의 차원수는 동일해야 한다.\n",
    "- 또한 `index`, `src`의 형태는 동일해야 한다.\n",
    "- 또한 `index` 내부에 저장된 스칼라 값은 `out`의 `dim`차원의 길이 보다는 작아야 한다.\n",
    "- 다음 예시는 1차원 텐서에서 1차원 텐서로 값을 선택하여 복사해 넣는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  0.,  0., 30.,  0., 50.,  0., 40.,  0., 20.])\n"
     ]
    }
   ],
   "source": [
    "out = torch.zeros(10, dtype=torch.float32)\n",
    "index = torch.tensor([0, 9, 3, 7, 5])\n",
    "dim = 0\n",
    "src = torch.tensor([10, 20, 30, 40, 50], dtype=torch.float32)\n",
    "\n",
    "print(torch.scatter(out, dim, index, src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음 예시는 2차원 행렬에서 2차원 행렬로 `scatter`한다.\n",
    "- `dim=0`이므로 `index`의 값이 `out` 행렬 위에서 값이 덮어써질 행 위치를 결정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2],\n",
      "        [2, 0]])\n",
      "tensor([[10., 20.],\n",
      "        [30., 40.]])\n",
      "tensor([[10., 40.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [30., 20.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "out = torch.eye(3)\n",
    "index = torch.tensor([[0, 2], [2, 0]])\n",
    "dim = 0\n",
    "src = torch.tensor([[10, 20],[30, 40]], dtype=torch.float32)\n",
    "\n",
    "print(index)\n",
    "print(src)\n",
    "print(torch.scatter(out, dim, index, src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음 예시또한 2차원 행렬에서 2차원 행렬로 `scatter`한다.\n",
    "- `dim=1`이므로 `index`의 값이 `out` 행렬 위에서 값이 덮어써질 열 위치를 결정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2],\n",
      "        [2, 0]])\n",
      "tensor([[10., 20.],\n",
      "        [30., 40.]])\n",
      "tensor([[10.,  0., 20.],\n",
      "        [40.,  1., 30.],\n",
      "        [ 0.,  0.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "out = torch.eye(3)\n",
    "index = torch.tensor([[0, 2], [2, 0]])\n",
    "dim = 1\n",
    "src = torch.tensor([[10, 20],[30, 40]], dtype=torch.float32)\n",
    "\n",
    "print(index)\n",
    "print(src)\n",
    "print(torch.scatter(out, dim, index, src))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
